This project was for a class, CNS/CS/Bi/Ph 187 (Neural Computation) for Fall 2016. This project investigated three aspects of neural networks: 

1) Using convolutional layers to replace fully-connected perceptron layers 
2) Testing effects of L1 and L2 regularization
3) Testing various activation layers, (tanh, sigmoid, relu) and their effect on accuracy

To test some code, execute
`python plot.py`