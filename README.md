This project was for a class, CNS/CS/Bi/Ph 187 (Neural Computation) for Fall 2016. This project investigated three aspects of neural networks: 

1) Using convolutional layers to replace fully-connected perceptron layers 
2) Testing effects of L1 and L2 regularization
3) Testing various activation layers, (tanh, sigmoid, relu) and their effect on accuracy

The writeup explaining the findings are located in `report.pdf`.

To test some code, execute
`python plot.py`